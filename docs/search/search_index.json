{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Local Cardinality Estimation (See [1]) The complete documentation can be found here . The submodule 'meta-collector' collects the several informations from the requested table and saves the information into a .json file The submodule 'sql-generator' uses the output of the meta-collector to create random SQL-Queries with the corresponding schema The submodule 'vectorizer' uses the output of the sql-generator to encode it into a vectors The submodule 'estimator' takes the encoded vectors and uses them on a neural network The submodule 'postrgres-evaluator' takes the sql-queries and executes them on the postgres-database to get the true cardinality For building the Documentation you need to execute the setup_doc.sh . This script installs the prerequisites if not already installed, builds the documentation and starts the documentation-server. References [1] Woltmann et al., Cardinality estimation with local deep learning models, aiDM@SIGMOD 2019 [2] Woltmann et al., Aggregate-based Training Phase for ML-based Cardinality Estimation, BTW 2021 [3] Woltmann et al., Machine Learning-based Cardinality Estimation in DBMS on Pre-Aggregated Data, arXiv 2020 Cite Please cite our papers if you use this code in your own work: @article{woltmann2019localdeep, title = {Cardinality estimation with local deep learning models}, author = {Woltmann, Lucas and Hartmann, Claudio and Thiele, Maik and Habich, Dirk and Lehner, Wolfgang}, booktitle = {Proceedings of the Second International Workshop on Exploiting Artificial Intelligence Techniques for Data Management}, series = {aiDM@SIGMOD '19}, year = {2019} } @article{woltmann2021aggregate, title={Aggregate-based Training Phase for ML-based Cardinality Estimation}, author={Woltmann, Lucas and Hartmann, Claudio and Habich, Dirk and Lehner, Wolfgang}, journal={BTW 2021}, year={2021}, publisher={Gesellschaft f{\\\"u}r Informatik, Bonn}, pages = {135-154}, doi = {10.18420/btw2021-07} } @article{woltmann2020cube, title={Machine Learning-based Cardinality Estimation in DBMS on Pre-Aggregated Data}, author={Woltmann, Lucas and Hartmann, Claudio and Habich, Dirk and Lehner, Wolfgang}, journal={arXiv preprint arXiv:2005.09367}, year={2020} }","title":"Home"},{"location":"#local-cardinality-estimation-see-1","text":"","title":"Local Cardinality Estimation (See [1])"},{"location":"#the-complete-documentation-can-be-found-here","text":"The submodule 'meta-collector' collects the several informations from the requested table and saves the information into a .json file The submodule 'sql-generator' uses the output of the meta-collector to create random SQL-Queries with the corresponding schema The submodule 'vectorizer' uses the output of the sql-generator to encode it into a vectors The submodule 'estimator' takes the encoded vectors and uses them on a neural network The submodule 'postrgres-evaluator' takes the sql-queries and executes them on the postgres-database to get the true cardinality For building the Documentation you need to execute the setup_doc.sh . This script installs the prerequisites if not already installed, builds the documentation and starts the documentation-server.","title":"The complete documentation can be found here."},{"location":"#references","text":"[1] Woltmann et al., Cardinality estimation with local deep learning models, aiDM@SIGMOD 2019 [2] Woltmann et al., Aggregate-based Training Phase for ML-based Cardinality Estimation, BTW 2021 [3] Woltmann et al., Machine Learning-based Cardinality Estimation in DBMS on Pre-Aggregated Data, arXiv 2020","title":"References"},{"location":"#cite","text":"Please cite our papers if you use this code in your own work: @article{woltmann2019localdeep, title = {Cardinality estimation with local deep learning models}, author = {Woltmann, Lucas and Hartmann, Claudio and Thiele, Maik and Habich, Dirk and Lehner, Wolfgang}, booktitle = {Proceedings of the Second International Workshop on Exploiting Artificial Intelligence Techniques for Data Management}, series = {aiDM@SIGMOD '19}, year = {2019} } @article{woltmann2021aggregate, title={Aggregate-based Training Phase for ML-based Cardinality Estimation}, author={Woltmann, Lucas and Hartmann, Claudio and Habich, Dirk and Lehner, Wolfgang}, journal={BTW 2021}, year={2021}, publisher={Gesellschaft f{\\\"u}r Informatik, Bonn}, pages = {135-154}, doi = {10.18420/btw2021-07} } @article{woltmann2020cube, title={Machine Learning-based Cardinality Estimation in DBMS on Pre-Aggregated Data}, author={Woltmann, Lucas and Hartmann, Claudio and Habich, Dirk and Lehner, Wolfgang}, journal={arXiv preprint arXiv:2005.09367}, year={2020} }","title":"Cite"},{"location":"databaseconnector/databaseconnector-class/","text":"database_connector.database_connector Database Objects class Database(Enum) Enum for the different supported databases. If you use MySQL, then use MARIADB as value here. DatabaseConnector Objects class DatabaseConnector() Class for DatabaseConnector. __init__ def __init__(database: Database, debug: bool = False) Initializer for the DatabaseConnector Arguments : debug : boolean whether to print additional information while processing connect def connect(config: Dict = None, config_file_path: str = None, sqlite_file_path: str = None) Wrapper method for connecting to the selected database. Arguments : host and port (default to host: localhost, port: 5432 if not given) for PostgreSQL or it has to be a dictionary with at least database, user and password and optionally host and port (default to host: localhost, port: 3306 if not given) for MariaDB. if not given: The config file path is needed and used for these settings. config : if given: It has to be a dictionary with at least db_name, user and password and optionally config_file_path : Path to the config file for PostgreSQL or MariaDB. sqlite_file_path : Path to the SQLite database file. connect_to_postgres def connect_to_postgres(config: Dict = None, config_file_path: str = \"config.yaml\") Connect to the postgres database with the given config. Arguments : and port (default to host: localhost, port: 5432 if not given) if not given: the config file 'config.yaml' is used for these settings .yaml/.yml file config : if given: it has to be a dictionary with at least db_name, user and password and optionally host config_file_path : path for the config-file -> only necessary if no config is given; needs to point on a connect_to_mariadb def connect_to_mariadb(config: Dict = None, config_file_path: str = \"config.yaml\") Connect to the postgres database with the given config. Arguments : and port (default to host: localhost, port: 3306 if not given) if not given: the config file 'config.yaml' is used for these settings .yaml/.yml file config : if given: it has to be a dictionary with at least db_name, user and password and optionally host config_file_path : path for the config-file -> only necessary if no config is given; needs to point on a connect_to_sqlite def connect_to_sqlite(database_file_path: str) Open connection to a sqlite database. Arguments : database_file_path : The path to the sqlite database file. close_database_connection def close_database_connection() close the connection to the database Returns : void execute def execute(sql_string: str) Method for executing a SQL-Query. Arguments : sql_string : The SQL-Query to execute fetchall def fetchall() Wrapper for fetchall method. fetchone def fetchone() Wrapper for fetchone method.","title":"DatabaseConnector Class"},{"location":"databaseconnector/databaseconnector-class/#database_connectordatabase_connector","text":"","title":"database_connector.database_connector"},{"location":"databaseconnector/databaseconnector-class/#database-objects","text":"class Database(Enum) Enum for the different supported databases. If you use MySQL, then use MARIADB as value here.","title":"Database Objects"},{"location":"databaseconnector/databaseconnector-class/#databaseconnector-objects","text":"class DatabaseConnector() Class for DatabaseConnector.","title":"DatabaseConnector Objects"},{"location":"databaseconnector/databaseconnector-class/#__init__","text":"def __init__(database: Database, debug: bool = False) Initializer for the DatabaseConnector Arguments : debug : boolean whether to print additional information while processing","title":"__init__"},{"location":"databaseconnector/databaseconnector-class/#connect","text":"def connect(config: Dict = None, config_file_path: str = None, sqlite_file_path: str = None) Wrapper method for connecting to the selected database. Arguments : host and port (default to host: localhost, port: 5432 if not given) for PostgreSQL or it has to be a dictionary with at least database, user and password and optionally host and port (default to host: localhost, port: 3306 if not given) for MariaDB. if not given: The config file path is needed and used for these settings. config : if given: It has to be a dictionary with at least db_name, user and password and optionally config_file_path : Path to the config file for PostgreSQL or MariaDB. sqlite_file_path : Path to the SQLite database file.","title":"connect"},{"location":"databaseconnector/databaseconnector-class/#connect_to_postgres","text":"def connect_to_postgres(config: Dict = None, config_file_path: str = \"config.yaml\") Connect to the postgres database with the given config. Arguments : and port (default to host: localhost, port: 5432 if not given) if not given: the config file 'config.yaml' is used for these settings .yaml/.yml file config : if given: it has to be a dictionary with at least db_name, user and password and optionally host config_file_path : path for the config-file -> only necessary if no config is given; needs to point on a","title":"connect_to_postgres"},{"location":"databaseconnector/databaseconnector-class/#connect_to_mariadb","text":"def connect_to_mariadb(config: Dict = None, config_file_path: str = \"config.yaml\") Connect to the postgres database with the given config. Arguments : and port (default to host: localhost, port: 3306 if not given) if not given: the config file 'config.yaml' is used for these settings .yaml/.yml file config : if given: it has to be a dictionary with at least db_name, user and password and optionally host config_file_path : path for the config-file -> only necessary if no config is given; needs to point on a","title":"connect_to_mariadb"},{"location":"databaseconnector/databaseconnector-class/#connect_to_sqlite","text":"def connect_to_sqlite(database_file_path: str) Open connection to a sqlite database. Arguments : database_file_path : The path to the sqlite database file.","title":"connect_to_sqlite"},{"location":"databaseconnector/databaseconnector-class/#close_database_connection","text":"def close_database_connection() close the connection to the database Returns : void","title":"close_database_connection"},{"location":"databaseconnector/databaseconnector-class/#execute","text":"def execute(sql_string: str) Method for executing a SQL-Query. Arguments : sql_string : The SQL-Query to execute","title":"execute"},{"location":"databaseconnector/databaseconnector-class/#fetchall","text":"def fetchall() Wrapper for fetchall method.","title":"fetchall"},{"location":"databaseconnector/databaseconnector-class/#fetchone","text":"def fetchone() Wrapper for fetchone method.","title":"fetchone"},{"location":"databaseconnector/databaseconnector-readme/","text":"DatabaseConnector This module contains the functions for connecting to the database and sending requests. As DBMS supported are PostgreSQL, MariaDB (MySQL) and SQLite. It is used in the MetaCollector and the DatabaseEvaluator. Usage If you want to instantiate this class, you need to have a configuration file (.yaml) or a configuration dict. This file/dict should look like: # PostgreSQL db_name: imdb user: postgres password: postgres host: localhost port: 5432 for PostgreSQL or like: # MariaDB database: imdb user: root password: maria host: localhost port: 3306 for MariaDB, respectively with your configuration for the DBMS. If you want to use SQLite, than you only have to give the path to the db file. Examples PostgreSQL: db_conn = DatabaseConnector(database=Database.POSTGRES) db_conn.connect(config_file_path=\"meta_collector/config_postgres.yaml\") MariaDB: db_conn = DatabaseConnector(database=Database.MARIADB) db_conn.connect(config_file_path=\"meta_collector/config_mariadb.yaml\") SQLite: db_conn = DatabaseConnector(database=Database.SQLITE) db_conn.connect(sqlite_file_path=\"E:/imdb.db\")","title":"DatabaseConnector Readme"},{"location":"databaseconnector/databaseconnector-readme/#databaseconnector","text":"This module contains the functions for connecting to the database and sending requests. As DBMS supported are PostgreSQL, MariaDB (MySQL) and SQLite. It is used in the MetaCollector and the DatabaseEvaluator.","title":"DatabaseConnector"},{"location":"databaseconnector/databaseconnector-readme/#usage","text":"If you want to instantiate this class, you need to have a configuration file (.yaml) or a configuration dict. This file/dict should look like: # PostgreSQL db_name: imdb user: postgres password: postgres host: localhost port: 5432 for PostgreSQL or like: # MariaDB database: imdb user: root password: maria host: localhost port: 3306 for MariaDB, respectively with your configuration for the DBMS. If you want to use SQLite, than you only have to give the path to the db file.","title":"Usage"},{"location":"databaseconnector/databaseconnector-readme/#examples","text":"PostgreSQL: db_conn = DatabaseConnector(database=Database.POSTGRES) db_conn.connect(config_file_path=\"meta_collector/config_postgres.yaml\") MariaDB: db_conn = DatabaseConnector(database=Database.MARIADB) db_conn.connect(config_file_path=\"meta_collector/config_mariadb.yaml\") SQLite: db_conn = DatabaseConnector(database=Database.SQLITE) db_conn.connect(sqlite_file_path=\"E:/imdb.db\")","title":"Examples"},{"location":"estimator/estimator-class/","text":"estimator.estimator Estimator Objects class Estimator() Class containing the neural network for cardinality estimation. The specifications of the neural network can be changed in 'config.yaml'. __init__ def __init__(config: Dict[str, Any] = None, config_file_path: str = \"config.yaml\", data: np.ndarray = None, model: Model = None, model_path: str = None, debug: bool = False) Initializer for the Estimator. Configuration options for the neural network are optionally passed via a config dict. It must contain at least the fields \"loss_function\", \"dropout\", \"learning_rate\", \"kernel_initializer\", \"activation_strategy\" and \"layer\". Arguments : if given: It must contain at least the fields \"loss_function\", \"dropout\", \"learning_rate\", \"kernel_initializer\", \"activation_strategy\" and \"layer\". if not given: the config file 'config.yaml' is used for these settings. at least \"x\" and \"y\" and optionally \"postgres_estimate\" as keys. The values have to be numpy.ndarray. For key \"x\" it should be the vectorized queries, for key \"y\" the true cardinalities in the same order and for optional key \"postgres_estimate\" the estimates of the postgres optimizer for the query. config : Only used if neither a model or a model_path is passed. config_file_path : path for the config-file -> only necessary if no config is given data : Optional parameter for giving the data for training and testing. If given it has to be a Dict with model : Option to pass a Model which can be used. model_path : Option to pass a path to a saved model in an .h5 file. debug : Boolean whether to print additional information while processing. get_model def get_model(len_input: int, override: bool = False) -> Model Function for creating the model of the neural network with the information from self.config Arguments : len_input : The size of the input vector. override : Whether an existing model should be overridden. Returns : The model for the neural network with the given properties. load_model def load_model(model_path: str) Method for loading an already existing model wich was saved to file. Arguments : model_path : Path to the file containing the model to load denormalize @staticmethod def denormalize(y, y_min: float, y_max: float) Arguments : y : tensor filled with values to denormalize y_min : minimum value for y y_max : maximum value for y Returns : tensor with denormalized values denormalize_np @staticmethod def denormalize_np(y: np.ndarray, y_min: float, y_max: float) -> np.ndarray Arguments : y : numpy-array filled with values to denormalize y_min : minimum value for y y_max : maximum value for y Returns : numpy-array with denormalized values load_data_file def load_data_file(file_path: str, override: bool = False) -> Dict[str, np.ndarray] Method for loading the data from file. Arguments : file_path : Path for the file where the data is stored. Has to be a .csv or .npy file. override : Boolean whether to override already existing data. Returns : The data which is set for the Estimator. set_data def set_data(loaded_data: np.ndarray, override: bool = False) Method for setting data and dependent values like max_card and input_length. Arguments : loaded_data : The data loaded from the file. override : Boolean whether to override already existing data. split_data def split_data(split: float = 0.9) Function to split the data into training- and test-set by a parameterized split value. Arguments : split : Percentage of the data going into training set. (split=0.9 means 90% of data is training set) train def train(epochs: int = 100, verbose: int = 1, shuffle: bool = True, batch_size: int = 32, validation_split: float = 0.1) -> Union[History, History] Method for training the before created Model. Arguments : epoch. (possibly) but enlarge training time, while bigger batches may lead to a less well trained network while training faster. training data, not the test data, and are reselected for every epoch. epochs : Number of epochs for training. verbose : How much information to print while training. 0 = silent, 1 = progress bar, 2 = one line per shuffle : Whether to shuffle the training data -> not necessary if split was done by numpy.random.choice() batch_size : Size for the batches -> Smaller batches may be able to train the neural network better validation_split : How much of the data should be taken as validation set -> these are taken from the Returns : Training history as dict. test def test() -> np.ndarray Let the trained neural network predict the test data. Returns : numpy-array containing the normalized predictions of the neural network for the test data predict def predict(data: np.ndarray) -> np.ndarray Let the trained neural network predict the given data. Arguments : data : numpy-array containing at least one vectorized query which should be predicted Returns : numpy-array containing the normalized predictions of the neural network for the given data run def run(data_file_path: str = None, epochs: int = 100, verbose: int = 1, shuffle: bool = True, batch_size: int = 32, validation_split: float = 0.1, override_model: bool = False, save_model: bool = True, save_model_file_path: str = \"model\") -> np.ndarray Method for a full run of the Estimator, with training and testing. Arguments : epoch. (possibly) but enlarge training time, while bigger batches may lead to a less well trained network while training faster. training data, not the test data, and are reselected for every epoch. should be saved. data_file_path : Optional path to saved data file. Only necessary if no data has been set before. epochs : Number of epochs for training. verbose : How much information to print while training. 0 = silent, 1 = progress bar, 2 = one line per shuffle : Whether to shuffle the training data -> not necessary if split was done by numpy.random.choice() batch_size : Size for the batches -> Smaller batches may be able to train the neural network better validation_split : How much of the data should be taken as validation set -> these are taken from the override_model : Whether to override a probably already existing model. save_model : Whether to save the trained model to file. save_model_file_path : When save_model==True this parameter is required to give the path where the model Returns : A numpy.ndarray containing the calculated q-error. save_model def save_model(filename: str = \"model\") Method for saving the Model to file. Arguments : filename) filename : Name of the file where the model should be stored. (Without file ending. \".h5\" is added to the","title":"Estimator Class"},{"location":"estimator/estimator-class/#estimatorestimator","text":"","title":"estimator.estimator"},{"location":"estimator/estimator-class/#estimator-objects","text":"class Estimator() Class containing the neural network for cardinality estimation. The specifications of the neural network can be changed in 'config.yaml'.","title":"Estimator Objects"},{"location":"estimator/estimator-class/#__init__","text":"def __init__(config: Dict[str, Any] = None, config_file_path: str = \"config.yaml\", data: np.ndarray = None, model: Model = None, model_path: str = None, debug: bool = False) Initializer for the Estimator. Configuration options for the neural network are optionally passed via a config dict. It must contain at least the fields \"loss_function\", \"dropout\", \"learning_rate\", \"kernel_initializer\", \"activation_strategy\" and \"layer\". Arguments : if given: It must contain at least the fields \"loss_function\", \"dropout\", \"learning_rate\", \"kernel_initializer\", \"activation_strategy\" and \"layer\". if not given: the config file 'config.yaml' is used for these settings. at least \"x\" and \"y\" and optionally \"postgres_estimate\" as keys. The values have to be numpy.ndarray. For key \"x\" it should be the vectorized queries, for key \"y\" the true cardinalities in the same order and for optional key \"postgres_estimate\" the estimates of the postgres optimizer for the query. config : Only used if neither a model or a model_path is passed. config_file_path : path for the config-file -> only necessary if no config is given data : Optional parameter for giving the data for training and testing. If given it has to be a Dict with model : Option to pass a Model which can be used. model_path : Option to pass a path to a saved model in an .h5 file. debug : Boolean whether to print additional information while processing.","title":"__init__"},{"location":"estimator/estimator-class/#get_model","text":"def get_model(len_input: int, override: bool = False) -> Model Function for creating the model of the neural network with the information from self.config Arguments : len_input : The size of the input vector. override : Whether an existing model should be overridden. Returns : The model for the neural network with the given properties.","title":"get_model"},{"location":"estimator/estimator-class/#load_model","text":"def load_model(model_path: str) Method for loading an already existing model wich was saved to file. Arguments : model_path : Path to the file containing the model to load","title":"load_model"},{"location":"estimator/estimator-class/#denormalize","text":"@staticmethod def denormalize(y, y_min: float, y_max: float) Arguments : y : tensor filled with values to denormalize y_min : minimum value for y y_max : maximum value for y Returns : tensor with denormalized values","title":"denormalize"},{"location":"estimator/estimator-class/#denormalize_np","text":"@staticmethod def denormalize_np(y: np.ndarray, y_min: float, y_max: float) -> np.ndarray Arguments : y : numpy-array filled with values to denormalize y_min : minimum value for y y_max : maximum value for y Returns : numpy-array with denormalized values","title":"denormalize_np"},{"location":"estimator/estimator-class/#load_data_file","text":"def load_data_file(file_path: str, override: bool = False) -> Dict[str, np.ndarray] Method for loading the data from file. Arguments : file_path : Path for the file where the data is stored. Has to be a .csv or .npy file. override : Boolean whether to override already existing data. Returns : The data which is set for the Estimator.","title":"load_data_file"},{"location":"estimator/estimator-class/#set_data","text":"def set_data(loaded_data: np.ndarray, override: bool = False) Method for setting data and dependent values like max_card and input_length. Arguments : loaded_data : The data loaded from the file. override : Boolean whether to override already existing data.","title":"set_data"},{"location":"estimator/estimator-class/#split_data","text":"def split_data(split: float = 0.9) Function to split the data into training- and test-set by a parameterized split value. Arguments : split : Percentage of the data going into training set. (split=0.9 means 90% of data is training set)","title":"split_data"},{"location":"estimator/estimator-class/#train","text":"def train(epochs: int = 100, verbose: int = 1, shuffle: bool = True, batch_size: int = 32, validation_split: float = 0.1) -> Union[History, History] Method for training the before created Model. Arguments : epoch. (possibly) but enlarge training time, while bigger batches may lead to a less well trained network while training faster. training data, not the test data, and are reselected for every epoch. epochs : Number of epochs for training. verbose : How much information to print while training. 0 = silent, 1 = progress bar, 2 = one line per shuffle : Whether to shuffle the training data -> not necessary if split was done by numpy.random.choice() batch_size : Size for the batches -> Smaller batches may be able to train the neural network better validation_split : How much of the data should be taken as validation set -> these are taken from the Returns : Training history as dict.","title":"train"},{"location":"estimator/estimator-class/#test","text":"def test() -> np.ndarray Let the trained neural network predict the test data. Returns : numpy-array containing the normalized predictions of the neural network for the test data","title":"test"},{"location":"estimator/estimator-class/#predict","text":"def predict(data: np.ndarray) -> np.ndarray Let the trained neural network predict the given data. Arguments : data : numpy-array containing at least one vectorized query which should be predicted Returns : numpy-array containing the normalized predictions of the neural network for the given data","title":"predict"},{"location":"estimator/estimator-class/#run","text":"def run(data_file_path: str = None, epochs: int = 100, verbose: int = 1, shuffle: bool = True, batch_size: int = 32, validation_split: float = 0.1, override_model: bool = False, save_model: bool = True, save_model_file_path: str = \"model\") -> np.ndarray Method for a full run of the Estimator, with training and testing. Arguments : epoch. (possibly) but enlarge training time, while bigger batches may lead to a less well trained network while training faster. training data, not the test data, and are reselected for every epoch. should be saved. data_file_path : Optional path to saved data file. Only necessary if no data has been set before. epochs : Number of epochs for training. verbose : How much information to print while training. 0 = silent, 1 = progress bar, 2 = one line per shuffle : Whether to shuffle the training data -> not necessary if split was done by numpy.random.choice() batch_size : Size for the batches -> Smaller batches may be able to train the neural network better validation_split : How much of the data should be taken as validation set -> these are taken from the override_model : Whether to override a probably already existing model. save_model : Whether to save the trained model to file. save_model_file_path : When save_model==True this parameter is required to give the path where the model Returns : A numpy.ndarray containing the calculated q-error.","title":"run"},{"location":"estimator/estimator-class/#save_model","text":"def save_model(filename: str = \"model\") Method for saving the Model to file. Arguments : filename) filename : Name of the file where the model should be stored. (Without file ending. \".h5\" is added to the","title":"save_model"},{"location":"estimator/estimator-readme/","text":"Estimator This module wraps the Neural Network which is used for the cardinality estimation. For every QuerySet there should be a new Estimator isntance. Usage Normally this submodule is called from main.py , however you may want to use it separately: You need a configuration file or dict for the NN. This should look like the following: ``` # Neural Network layer: 512 256 loss_function: q_loss dropout: 0.2 learning_rate: 0.0001 kernel_initializer: normal activation_strategy: relu len_input: ``` It contains information about the layers of the NN. You can add more (fully-connected) layers here. The len_input can be given, but is optional. If not given, this value is calculated from the data. You need a file containing the vectorized queries: 1,0,0,1,0,0,1,0.946902654867256666,0,1,0,0.554621848739495826,134163798,0.651576470484740322,0.745803338052605902 0,0,1,0.5,1,0,0,0.707964601769911495,1,0,1,0.0714285714285714246,134163798,0.552436280887511844,0.387697419969840307 1,0,1,0.5,1,1,0,0.548672566371681381,1,0,1,0.911764705882352922,134163798,0.9658556575333751,0.981214080678194711 0,1,1,1,0,1,0,0.39823008849557523,1,0,0,0.260504201680672287,134163798,0.715437781548679874,0.651506384900475854 0,0,1,1,1,0,1,0.283185840707964598,1,0,1,0.172268907563025209,134163798,0.767619189647782418,0.7410491653476593 0,1,1,1,1,0,0,0.477876106194690287,1,0,1,0.924369747899159711,134163798,0.973278750577822982,0.920647733098342469 1,0,1,1,0,1,1,0.336283185840707988,1,0,0,0.0798319327731092376,134163798,0.314815867372580604,0.418093768713123426 0,0,1,1,1,0,1,0.584070796460177011,0,0,1,0.626050420168067223,134163798,0.569767811257714807,0.141016173481957524 Where the last three values must be the maximum cardinality for the QuerySet, the normalized estimated cardinality and normalized true cardinality (in this order). This file could either be a .csv or a .npy file with this format. You can train the NN on this data and save it afterwards to file for reusage. The trained NN should now be capable of estimating cardinalities for the given QuerySet.","title":"Estimator Readme"},{"location":"estimator/estimator-readme/#estimator","text":"This module wraps the Neural Network which is used for the cardinality estimation. For every QuerySet there should be a new Estimator isntance.","title":"Estimator"},{"location":"estimator/estimator-readme/#usage","text":"Normally this submodule is called from main.py , however you may want to use it separately: You need a configuration file or dict for the NN. This should look like the following: ``` # Neural Network layer: 512 256 loss_function: q_loss dropout: 0.2 learning_rate: 0.0001 kernel_initializer: normal activation_strategy: relu len_input: ``` It contains information about the layers of the NN. You can add more (fully-connected) layers here. The len_input can be given, but is optional. If not given, this value is calculated from the data. You need a file containing the vectorized queries: 1,0,0,1,0,0,1,0.946902654867256666,0,1,0,0.554621848739495826,134163798,0.651576470484740322,0.745803338052605902 0,0,1,0.5,1,0,0,0.707964601769911495,1,0,1,0.0714285714285714246,134163798,0.552436280887511844,0.387697419969840307 1,0,1,0.5,1,1,0,0.548672566371681381,1,0,1,0.911764705882352922,134163798,0.9658556575333751,0.981214080678194711 0,1,1,1,0,1,0,0.39823008849557523,1,0,0,0.260504201680672287,134163798,0.715437781548679874,0.651506384900475854 0,0,1,1,1,0,1,0.283185840707964598,1,0,1,0.172268907563025209,134163798,0.767619189647782418,0.7410491653476593 0,1,1,1,1,0,0,0.477876106194690287,1,0,1,0.924369747899159711,134163798,0.973278750577822982,0.920647733098342469 1,0,1,1,0,1,1,0.336283185840707988,1,0,0,0.0798319327731092376,134163798,0.314815867372580604,0.418093768713123426 0,0,1,1,1,0,1,0.584070796460177011,0,0,1,0.626050420168067223,134163798,0.569767811257714807,0.141016173481957524 Where the last three values must be the maximum cardinality for the QuerySet, the normalized estimated cardinality and normalized true cardinality (in this order). This file could either be a .csv or a .npy file with this format. You can train the NN on this data and save it afterwards to file for reusage. The trained NN should now be capable of estimating cardinalities for the given QuerySet.","title":"Usage"},{"location":"meta-collector/meta-collector-class/","text":"meta_collector.meta_collector CreationMode Objects class CreationMode(Enum) Enum for the different possibilities to use the MetaCollector. 0 -> don't create table, 1 -> create temporary table, 2 -> create permanent table MetaCollector Objects class MetaCollector() Class for MetaCollector. __init__ def __init__(database_connector: DatabaseConnector, debug: bool = False) Initializer for the MetaCollector Arguments : database_connector : The connector to the used database. debug : boolean whether to print additional information while processing get_columns_data_postgres def get_columns_data_postgres(table_names: List[str or Tuple[str, str]], columns: List[str]) -> List[Tuple[str, str, str, Tuple[int, int, int], Dict[str, LabelEncoder], str]] Get column-name and datatype for the requested columns of the corresponding tables for PostgreSQL and MariaDB. Arguments : second place, to join. table_names : List of names of tables, as strings or tuples containing table-name in first and alias in columns : Columns to project on. Returns : A list containing the name of the column, the table alias (if existent, else the table-name), the get_columns_data_sqlite def get_columns_data_sqlite(table_names: List[str or Tuple[str, str]], columns: List[str]) -> List[Tuple[str, str, str, Tuple[int, int, int], Dict[str, LabelEncoder], str]] Get column-name and datatype for the requested columns of the corresponding tables for SQLite. Arguments : second place, to join. table_names : List of names of tables, as strings or tuples containing table-name in first and alias in columns : Columns to project on. Returns : A list containing the name of the column, the table alias (if existent, else the table-name), the collect_min_max_step def collect_min_max_step(tablename: str, column: Tuple[str, str]) -> (Tuple[int, int, int], Dict) After collecting the datatype information for the columns this function returns the min and max values for the meta-table and the encoders. Arguments : tablename : String containing the name of the table where to find the column column : a tuple containing the name and the datatype for the column, each as string Returns : first: dictionary with the attribute-name as key and a tuple containing min-value, max-value and get_max_card def get_max_card(table_names: List[str or Tuple[str, str]], join_atts: List[str or Tuple[str, str]] = None) -> int Get the size of the join-table without any selections, the so called max-card. Arguments : second place, to join. join the tables on. -> is optional, because there is no join if there is only one table and so there would be no join-attribute needed in that case table_names : List of names of tables, as strings or tuples containing table-name in first and alias in join_atts : List of attributes, as strings or tuples containing the two attributes to join with '=', to Returns : setup_view def setup_view(table_names: List[str or Tuple[str, str]], columns_types: List[Tuple], join_atts: List[str or Tuple[str, str]] = None, cube: bool = False, mode: CreationMode = CreationMode.NONE) -> (List[Tuple[str, str]], int) Create the tables tmpview and if cube==True also tmpview_cube containing the metadata for the given tables joined on the attributes and projected on the columns. Arguments : second place, to join. join the tables on. -> is optional, because there is no join if there is only one table and so there would be no join-attribute needed in that case table_names : List of names of tables, as strings or tuples containing table-name in first and alias in columns_types : columns to project on join_atts : List of attributes, as strings or tuples containing the two attributes to join with '=', to cube : boolean whether to create the *_cube table, too mode : see CreationMode-Enum Returns : first: a list of tuples containing the name and the datatype for the columns, each as string setup_cube_view def setup_cube_view(new_table_name: str, columns) Create the table tmpview_cube if cube==True containing the metadata for the given tables joined on the attributes and projected on the columns. Arguments : new_table_name : The name of the before created table. columns : Columns to project on. get_meta def get_meta(table_names: List[str or Tuple[str, str]], columns: List[str], join_atts: List[str or Tuple[str, str]] = None, mode: CreationMode = CreationMode.NONE, save: bool = True, save_file_name: str = None, batchmode: bool = False, cube: bool = False) -> Dict Method for the whole process of collecting the meta-information for the given tables joined on the given attributes and projected on the given columns. Arguments : second place, to join. join the tables on. -> is optional, because there is no join if there is only one table and so there would be no join-attribute needed in that case mode table_names : List of names of tables, as strings or tuples containing table-name in first and alias in columns : List of names of columns, as strings, to project on. join_atts : List of attributes, as strings or tuples containing the two attributes to join with '=', to save : boolean whether to save the meta-information to file save_file_name : name for the save-file for the meta_information -> not needed if save==False batchmode : whether the meta data is collected in batches or not -> connection to db held open if batch mode : see CreationMode-Enum cube : Whether to create the _cube table additionally (only for CreationMode Temporary or Permanent). Returns : dictionary containing the meta-information get_meta_from_file def get_meta_from_file(file_path: str, save: bool = True, save_file_path: str = None, mode: CreationMode = CreationMode.NONE, override: bool = True) -> Dict[int, any] Method for collecting meta data for the information given in a file from QueryParser or at least a file formatted like this. Arguments : file_path : Path to the file. Format has to be the same like the output of QueryParser save : Whether to save the information to file or not. -> It is recommended to do so. save_file_path : Optional path for the save-file. mode : see CreationMode-Enum override : Whether to override an already existing meta_information file. Returns : The solution dict. save_meta def save_meta(meta_dict: Dict, file_name: str = \"meta_information\", mode: str = \"w\") Method for saving the meta-information to file. Arguments : meta_dict : the dictionary containing the meta-information to save file_name : the name (without file-type) for the save-file mode : The mode to open the file. Some common possibilities are 'w', 'w+', 'r', 'a', 'a+'","title":"Meta-Collector Class"},{"location":"meta-collector/meta-collector-class/#meta_collectormeta_collector","text":"","title":"meta_collector.meta_collector"},{"location":"meta-collector/meta-collector-class/#creationmode-objects","text":"class CreationMode(Enum) Enum for the different possibilities to use the MetaCollector. 0 -> don't create table, 1 -> create temporary table, 2 -> create permanent table","title":"CreationMode Objects"},{"location":"meta-collector/meta-collector-class/#metacollector-objects","text":"class MetaCollector() Class for MetaCollector.","title":"MetaCollector Objects"},{"location":"meta-collector/meta-collector-class/#__init__","text":"def __init__(database_connector: DatabaseConnector, debug: bool = False) Initializer for the MetaCollector Arguments : database_connector : The connector to the used database. debug : boolean whether to print additional information while processing","title":"__init__"},{"location":"meta-collector/meta-collector-class/#get_columns_data_postgres","text":"def get_columns_data_postgres(table_names: List[str or Tuple[str, str]], columns: List[str]) -> List[Tuple[str, str, str, Tuple[int, int, int], Dict[str, LabelEncoder], str]] Get column-name and datatype for the requested columns of the corresponding tables for PostgreSQL and MariaDB. Arguments : second place, to join. table_names : List of names of tables, as strings or tuples containing table-name in first and alias in columns : Columns to project on. Returns : A list containing the name of the column, the table alias (if existent, else the table-name), the","title":"get_columns_data_postgres"},{"location":"meta-collector/meta-collector-class/#get_columns_data_sqlite","text":"def get_columns_data_sqlite(table_names: List[str or Tuple[str, str]], columns: List[str]) -> List[Tuple[str, str, str, Tuple[int, int, int], Dict[str, LabelEncoder], str]] Get column-name and datatype for the requested columns of the corresponding tables for SQLite. Arguments : second place, to join. table_names : List of names of tables, as strings or tuples containing table-name in first and alias in columns : Columns to project on. Returns : A list containing the name of the column, the table alias (if existent, else the table-name), the","title":"get_columns_data_sqlite"},{"location":"meta-collector/meta-collector-class/#collect_min_max_step","text":"def collect_min_max_step(tablename: str, column: Tuple[str, str]) -> (Tuple[int, int, int], Dict) After collecting the datatype information for the columns this function returns the min and max values for the meta-table and the encoders. Arguments : tablename : String containing the name of the table where to find the column column : a tuple containing the name and the datatype for the column, each as string Returns : first: dictionary with the attribute-name as key and a tuple containing min-value, max-value and","title":"collect_min_max_step"},{"location":"meta-collector/meta-collector-class/#get_max_card","text":"def get_max_card(table_names: List[str or Tuple[str, str]], join_atts: List[str or Tuple[str, str]] = None) -> int Get the size of the join-table without any selections, the so called max-card. Arguments : second place, to join. join the tables on. -> is optional, because there is no join if there is only one table and so there would be no join-attribute needed in that case table_names : List of names of tables, as strings or tuples containing table-name in first and alias in join_atts : List of attributes, as strings or tuples containing the two attributes to join with '=', to Returns :","title":"get_max_card"},{"location":"meta-collector/meta-collector-class/#setup_view","text":"def setup_view(table_names: List[str or Tuple[str, str]], columns_types: List[Tuple], join_atts: List[str or Tuple[str, str]] = None, cube: bool = False, mode: CreationMode = CreationMode.NONE) -> (List[Tuple[str, str]], int) Create the tables tmpview and if cube==True also tmpview_cube containing the metadata for the given tables joined on the attributes and projected on the columns. Arguments : second place, to join. join the tables on. -> is optional, because there is no join if there is only one table and so there would be no join-attribute needed in that case table_names : List of names of tables, as strings or tuples containing table-name in first and alias in columns_types : columns to project on join_atts : List of attributes, as strings or tuples containing the two attributes to join with '=', to cube : boolean whether to create the *_cube table, too mode : see CreationMode-Enum Returns : first: a list of tuples containing the name and the datatype for the columns, each as string","title":"setup_view"},{"location":"meta-collector/meta-collector-class/#setup_cube_view","text":"def setup_cube_view(new_table_name: str, columns) Create the table tmpview_cube if cube==True containing the metadata for the given tables joined on the attributes and projected on the columns. Arguments : new_table_name : The name of the before created table. columns : Columns to project on.","title":"setup_cube_view"},{"location":"meta-collector/meta-collector-class/#get_meta","text":"def get_meta(table_names: List[str or Tuple[str, str]], columns: List[str], join_atts: List[str or Tuple[str, str]] = None, mode: CreationMode = CreationMode.NONE, save: bool = True, save_file_name: str = None, batchmode: bool = False, cube: bool = False) -> Dict Method for the whole process of collecting the meta-information for the given tables joined on the given attributes and projected on the given columns. Arguments : second place, to join. join the tables on. -> is optional, because there is no join if there is only one table and so there would be no join-attribute needed in that case mode table_names : List of names of tables, as strings or tuples containing table-name in first and alias in columns : List of names of columns, as strings, to project on. join_atts : List of attributes, as strings or tuples containing the two attributes to join with '=', to save : boolean whether to save the meta-information to file save_file_name : name for the save-file for the meta_information -> not needed if save==False batchmode : whether the meta data is collected in batches or not -> connection to db held open if batch mode : see CreationMode-Enum cube : Whether to create the _cube table additionally (only for CreationMode Temporary or Permanent). Returns : dictionary containing the meta-information","title":"get_meta"},{"location":"meta-collector/meta-collector-class/#get_meta_from_file","text":"def get_meta_from_file(file_path: str, save: bool = True, save_file_path: str = None, mode: CreationMode = CreationMode.NONE, override: bool = True) -> Dict[int, any] Method for collecting meta data for the information given in a file from QueryParser or at least a file formatted like this. Arguments : file_path : Path to the file. Format has to be the same like the output of QueryParser save : Whether to save the information to file or not. -> It is recommended to do so. save_file_path : Optional path for the save-file. mode : see CreationMode-Enum override : Whether to override an already existing meta_information file. Returns : The solution dict.","title":"get_meta_from_file"},{"location":"meta-collector/meta-collector-class/#save_meta","text":"def save_meta(meta_dict: Dict, file_name: str = \"meta_information\", mode: str = \"w\") Method for saving the meta-information to file. Arguments : meta_dict : the dictionary containing the meta-information to save file_name : the name (without file-type) for the save-file mode : The mode to open the file. Some common possibilities are 'w', 'w+', 'r', 'a', 'a+'","title":"save_meta"},{"location":"meta-collector/meta-collector-readme/","text":"MetaCollector This module uses the output of the QueryParser and adds additional meta information from the database which is needed to generate the SQL-Queries. For the connection to the DBMS the DatabaseConnector is used. Usage Normally this submodule is called from main.py, however you may want to use it separately: You need a DatabaseConnector with an established connection to the DBMS. You need the output of the QueryParser like: 0: join_attributes: - t.id=mc.movie_id - t.id=mi_idx.movie_id selection_attributes: - mc.company_type_id - mi_idx.info_type_id - t.production_year table_names: - - movie_companies - mc - - movie_info_idx - mi_idx - - title - t 1: join_attributes: - t.id=mc.movie_id - t.id=mk.movie_id selection_attributes: - mc.company_type_id - mk.keyword_id - t.production_year table_names: - - movie_companies - mc - - movie_keyword - mk - - title - t You can save the results to file like: 0: columns: - - company_type_id - mc - integer - - 1 - 2 - 1 - {} - '' - - info_type_id - mi_idx - integer - - 1 - 113 - 1 - {} - '' - - production_year - t - integer - - 1874 - 2115 - 1 - {} - '' join_attributes: - t.id=mc.movie_id - t.id=mi_idx.movie_id max_card: 134952836 table_names: - - movie_companies - mc - - movie_info_idx - mi_idx - - title - t","title":"Meta-Collector Readme"},{"location":"meta-collector/meta-collector-readme/#metacollector","text":"This module uses the output of the QueryParser and adds additional meta information from the database which is needed to generate the SQL-Queries. For the connection to the DBMS the DatabaseConnector is used.","title":"MetaCollector"},{"location":"meta-collector/meta-collector-readme/#usage","text":"Normally this submodule is called from main.py, however you may want to use it separately: You need a DatabaseConnector with an established connection to the DBMS. You need the output of the QueryParser like: 0: join_attributes: - t.id=mc.movie_id - t.id=mi_idx.movie_id selection_attributes: - mc.company_type_id - mi_idx.info_type_id - t.production_year table_names: - - movie_companies - mc - - movie_info_idx - mi_idx - - title - t 1: join_attributes: - t.id=mc.movie_id - t.id=mk.movie_id selection_attributes: - mc.company_type_id - mk.keyword_id - t.production_year table_names: - - movie_companies - mc - - movie_keyword - mk - - title - t You can save the results to file like: 0: columns: - - company_type_id - mc - integer - - 1 - 2 - 1 - {} - '' - - info_type_id - mi_idx - integer - - 1 - 113 - 1 - {} - '' - - production_year - t - integer - - 1874 - 2115 - 1 - {} - '' join_attributes: - t.id=mc.movie_id - t.id=mi_idx.movie_id max_card: 134952836 table_names: - - movie_companies - mc - - movie_info_idx - mi_idx - - title - t","title":"Usage"},{"location":"querycommunicator/querycommunicator-class/","text":"query_communicator.query_communicator QueryCommunicator Objects class QueryCommunicator() Class for oberserving the generation and evaluation of queries, in order to have nullqueryfree set of queries if needed. Manages the communication between Evaluator and SQL Generator to get the required amount of queries if possible. The SQL_Generator itself is not able to find nullqueries, that are caused by a valid combination of attributes, which just don't match any data of the database. Vice Versa, the Evaluator is not able to generate new queries, if there are nullqueries. get_queries def get_queries(database_connector: DatabaseConnector, save_file_path: str, query_number: int) Function for generating queries and their cardinalities if nullqueries are allowed. Saves generated queries in ../assets/queries_with_cardinalities.csv Arguments : query_number : number of queries to generate save_file_path : path to save the finished queries with their cardinalities database_connector : Handles the database connection to the desired database. Returns : get_nullfree_queries def get_nullfree_queries(query_number: int, save_file_path: str, database_connector: DatabaseConnector) Function that generates given number queries and their cardinalities which are not zero. There will be less queries then requested, if unavoidable. Arguments : query_number : number of queries to generate save_file_path : path to save the finished queries with their cardinalities database_connector : Handles the database connection to the desired database. Returns : list of remained Queries produce_queries def produce_queries(database_connector: DatabaseConnector, query_number: int = 10, nullqueries: bool = False, save_file_path: str = 'assets/reduced_queries_with_cardinalities.csv') Main function to produce the queries and return the correct csv file, depending if nullqueries are wanted or not Arguments : save_file_path : Path to save the finished query file nullqueries : decide whether to generate nullqueries or not, default: no nullqueries query_number : count of queries that are generated per meta file entry database_connector : Connector for the database connection, depending on the database system you are using Returns :","title":"Index"},{"location":"querycommunicator/querycommunicator-class/#query_communicatorquery_communicator","text":"","title":"query_communicator.query_communicator"},{"location":"querycommunicator/querycommunicator-class/#querycommunicator-objects","text":"class QueryCommunicator() Class for oberserving the generation and evaluation of queries, in order to have nullqueryfree set of queries if needed. Manages the communication between Evaluator and SQL Generator to get the required amount of queries if possible. The SQL_Generator itself is not able to find nullqueries, that are caused by a valid combination of attributes, which just don't match any data of the database. Vice Versa, the Evaluator is not able to generate new queries, if there are nullqueries.","title":"QueryCommunicator Objects"},{"location":"querycommunicator/querycommunicator-class/#get_queries","text":"def get_queries(database_connector: DatabaseConnector, save_file_path: str, query_number: int) Function for generating queries and their cardinalities if nullqueries are allowed. Saves generated queries in ../assets/queries_with_cardinalities.csv Arguments : query_number : number of queries to generate save_file_path : path to save the finished queries with their cardinalities database_connector : Handles the database connection to the desired database. Returns :","title":"get_queries"},{"location":"querycommunicator/querycommunicator-class/#get_nullfree_queries","text":"def get_nullfree_queries(query_number: int, save_file_path: str, database_connector: DatabaseConnector) Function that generates given number queries and their cardinalities which are not zero. There will be less queries then requested, if unavoidable. Arguments : query_number : number of queries to generate save_file_path : path to save the finished queries with their cardinalities database_connector : Handles the database connection to the desired database. Returns : list of remained Queries","title":"get_nullfree_queries"},{"location":"querycommunicator/querycommunicator-class/#produce_queries","text":"def produce_queries(database_connector: DatabaseConnector, query_number: int = 10, nullqueries: bool = False, save_file_path: str = 'assets/reduced_queries_with_cardinalities.csv') Main function to produce the queries and return the correct csv file, depending if nullqueries are wanted or not Arguments : save_file_path : Path to save the finished query file nullqueries : decide whether to generate nullqueries or not, default: no nullqueries query_number : count of queries that are generated per meta file entry database_connector : Connector for the database connection, depending on the database system you are using Returns :","title":"produce_queries"},{"location":"querycommunicator/querycommunicator-class/databaseevaluator-class/","text":"","title":"DatabaseEvaluator Class"},{"location":"querycommunicator/querycommunicator-class/databaseevaluator-readme/","text":"Database Evaluator This sub(sub)module is part of the query communicator and uses the output from the sql-generator submodule to evaluate the generated queries against a database (currently only PostgreSQL is supported) und get the corresponding estimated and true cardinality of each query. Queries and cardinalities are input to the vectorizer submodule. Usage Usually this submodule is called from the query communicator and does not communicate directly with the main.py, however you may want to use it seperatly: First, you need a CSV file (semicolon separated) with the queries and meta data. querySetID;query;encodings;max_card;min_max_step 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id<2 AND mi_idx.info_type_id>=30 AND t.production_year>=2050;{};134163798;{'company_type_id': [1, 2, 1], 'info_type_id': [1, 113, 1], 'production_year': [1878, 2115, 1]} 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id<1 AND mi_idx.info_type_id=71 AND t.production_year<=2109;{};134163798;{'company_type_id': [1, 2, 1], 'info_type_id': [1, 113, 1], 'production_year': [1878, 2115, 1]} 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id<2 AND mi_idx.info_type_id=107 AND t.production_year>2009;{};134163798;{'company_type_id': [1, 2, 1], 'info_type_id': [1, 113, 1], 'production_year': [1878, 2115, 1]} 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id=1 AND mi_idx.info_type_id<80 AND t.production_year<=1894;{};134163798;{'company_type_id': [1, 2, 1], 'info_type_id': [1, 113, 1], 'production_year': [1878, 2115, 1]} 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id<=1 AND mi_idx.info_type_id!=62 AND t.production_year<=2094;{};134163798;{'company_type_id': [1, 2, 1], 'info_type_id': [1, 113, 1], 'production_year': [1878, 2115, 1]} 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id>=2 AND mi_idx.info_type_id>45 AND t.production_year<1939;{};134163798;{'company_type_id': [1, 2, 1], 'info_type_id': [1, 113, 1], 'production_year': [1878, 2115, 1]} 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id=2 AND mi_idx.info_type_id<=32 AND t.production_year<=1918;{};134163798;{'company_type_id': [1, 2, 1], 'info_type_id': [1, 113, 1], 'production_year': [1878, 2115, 1]} 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id>=2 AND mi_idx.info_type_id<54 AND t.production_year<=2097;{};134163798;{'company_type_id': [1, 2, 1], 'info_type_id': [1, 113, 1], 'production_year': [1878, 2115, 1]} 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id<=2 AND mi_idx.info_type_id>=38 AND t.production_year<1896;{};134163798;{'company_type_id': [1, 2, 1], 'info_type_id': [1, 113, 1], 'production_year': [1878, 2115, 1]} 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id=2 AND mi_idx.info_type_id<=66 AND t.production_year=2026;{};134163798;{'company_type_id': [1, 2, 1], 'info_type_id': [1, 113, 1], 'production_year': [1878, 2115, 1]} 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id=1 AND mk.keyword_id>=117023 AND t.production_year<=1894;{};63056995;{'company_type_id': [1, 2, 1], 'keyword_id': [1, 236627, 1], 'production_year': [1878, 2115, 1]} 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id<1 AND mk.keyword_id>=134591 AND t.production_year<2015;{};63056995;{'company_type_id': [1, 2, 1], 'keyword_id': [1, 236627, 1], 'production_year': [1878, 2115, 1]} 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id>=1 AND mk.keyword_id<35239 AND t.production_year<=1896;{};63056995;{'company_type_id': [1, 2, 1], 'keyword_id': [1, 236627, 1], 'production_year': [1878, 2115, 1]} 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id<2 AND mk.keyword_id<=35888 AND t.production_year!=2020;{};63056995;{'company_type_id': [1, 2, 1], 'keyword_id': [1, 236627, 1], 'production_year': [1878, 2115, 1]} 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id<=1 AND mk.keyword_id=20641 AND t.production_year=1889;{};63056995;{'company_type_id': [1, 2, 1], 'keyword_id': [1, 236627, 1], 'production_year': [1878, 2115, 1]} 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id>2 AND mk.keyword_id!=104628 AND t.production_year>=1940;{};63056995;{'company_type_id': [1, 2, 1], 'keyword_id': [1, 236627, 1], 'production_year': [1878, 2115, 1]} 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id=1 AND mk.keyword_id!=176282 AND t.production_year=2055;{};63056995;{'company_type_id': [1, 2, 1], 'keyword_id': [1, 236627, 1], 'production_year': [1878, 2115, 1]} 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id=2 AND mk.keyword_id>196933 AND t.production_year=1907;{};63056995;{'company_type_id': [1, 2, 1], 'keyword_id': [1, 236627, 1], 'production_year': [1878, 2115, 1]} 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id!=1 AND mk.keyword_id<19712 AND t.production_year<1980;{};63056995;{'company_type_id': [1, 2, 1], 'keyword_id': [1, 236627, 1], 'production_year': [1878, 2115, 1]} 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id>1 AND mk.keyword_id<=186471 AND t.production_year<2110;{};63056995;{'company_type_id': [1, 2, 1], 'keyword_id': [1, 236627, 1], 'production_year': [1878, 2115, 1]} Alternativly also a SQL-file with just the queries would work as input, but be aware that you have to add the meta data later then for further processing. Initialize the Database Evaluator and call the get_cardinalities() function. python evaluator = DatabaseEvaluator(input_file_path=inter_file_path, database_connector=database_connector) evaluator.get_cardinalities(eliminate_null_queries=False, save_file_path=save_file_path.split(\".\")[0], query_number=query_number) Note, that inter_file_path holding just the same string as save_file_path with a additional \"inter_\"-praefix (file used in the SQL-Generator module). Per default all intermediate products are saved in the asset folder. Be also beware of the hints in the parent module (QueryCommunicator) documentation .","title":"DatabaseEvaluator Readme"},{"location":"querycommunicator/querycommunicator-class/databaseevaluator-readme/#database-evaluator","text":"This sub(sub)module is part of the query communicator and uses the output from the sql-generator submodule to evaluate the generated queries against a database (currently only PostgreSQL is supported) und get the corresponding estimated and true cardinality of each query. Queries and cardinalities are input to the vectorizer submodule.","title":"Database Evaluator"},{"location":"querycommunicator/querycommunicator-class/databaseevaluator-readme/#usage","text":"Usually this submodule is called from the query communicator and does not communicate directly with the main.py, however you may want to use it seperatly: First, you need a CSV file (semicolon separated) with the queries and meta data. querySetID;query;encodings;max_card;min_max_step 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id<2 AND mi_idx.info_type_id>=30 AND t.production_year>=2050;{};134163798;{'company_type_id': [1, 2, 1], 'info_type_id': [1, 113, 1], 'production_year': [1878, 2115, 1]} 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id<1 AND mi_idx.info_type_id=71 AND t.production_year<=2109;{};134163798;{'company_type_id': [1, 2, 1], 'info_type_id': [1, 113, 1], 'production_year': [1878, 2115, 1]} 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id<2 AND mi_idx.info_type_id=107 AND t.production_year>2009;{};134163798;{'company_type_id': [1, 2, 1], 'info_type_id': [1, 113, 1], 'production_year': [1878, 2115, 1]} 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id=1 AND mi_idx.info_type_id<80 AND t.production_year<=1894;{};134163798;{'company_type_id': [1, 2, 1], 'info_type_id': [1, 113, 1], 'production_year': [1878, 2115, 1]} 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id<=1 AND mi_idx.info_type_id!=62 AND t.production_year<=2094;{};134163798;{'company_type_id': [1, 2, 1], 'info_type_id': [1, 113, 1], 'production_year': [1878, 2115, 1]} 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id>=2 AND mi_idx.info_type_id>45 AND t.production_year<1939;{};134163798;{'company_type_id': [1, 2, 1], 'info_type_id': [1, 113, 1], 'production_year': [1878, 2115, 1]} 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id=2 AND mi_idx.info_type_id<=32 AND t.production_year<=1918;{};134163798;{'company_type_id': [1, 2, 1], 'info_type_id': [1, 113, 1], 'production_year': [1878, 2115, 1]} 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id>=2 AND mi_idx.info_type_id<54 AND t.production_year<=2097;{};134163798;{'company_type_id': [1, 2, 1], 'info_type_id': [1, 113, 1], 'production_year': [1878, 2115, 1]} 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id<=2 AND mi_idx.info_type_id>=38 AND t.production_year<1896;{};134163798;{'company_type_id': [1, 2, 1], 'info_type_id': [1, 113, 1], 'production_year': [1878, 2115, 1]} 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id=2 AND mi_idx.info_type_id<=66 AND t.production_year=2026;{};134163798;{'company_type_id': [1, 2, 1], 'info_type_id': [1, 113, 1], 'production_year': [1878, 2115, 1]} 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id=1 AND mk.keyword_id>=117023 AND t.production_year<=1894;{};63056995;{'company_type_id': [1, 2, 1], 'keyword_id': [1, 236627, 1], 'production_year': [1878, 2115, 1]} 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id<1 AND mk.keyword_id>=134591 AND t.production_year<2015;{};63056995;{'company_type_id': [1, 2, 1], 'keyword_id': [1, 236627, 1], 'production_year': [1878, 2115, 1]} 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id>=1 AND mk.keyword_id<35239 AND t.production_year<=1896;{};63056995;{'company_type_id': [1, 2, 1], 'keyword_id': [1, 236627, 1], 'production_year': [1878, 2115, 1]} 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id<2 AND mk.keyword_id<=35888 AND t.production_year!=2020;{};63056995;{'company_type_id': [1, 2, 1], 'keyword_id': [1, 236627, 1], 'production_year': [1878, 2115, 1]} 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id<=1 AND mk.keyword_id=20641 AND t.production_year=1889;{};63056995;{'company_type_id': [1, 2, 1], 'keyword_id': [1, 236627, 1], 'production_year': [1878, 2115, 1]} 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id>2 AND mk.keyword_id!=104628 AND t.production_year>=1940;{};63056995;{'company_type_id': [1, 2, 1], 'keyword_id': [1, 236627, 1], 'production_year': [1878, 2115, 1]} 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id=1 AND mk.keyword_id!=176282 AND t.production_year=2055;{};63056995;{'company_type_id': [1, 2, 1], 'keyword_id': [1, 236627, 1], 'production_year': [1878, 2115, 1]} 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id=2 AND mk.keyword_id>196933 AND t.production_year=1907;{};63056995;{'company_type_id': [1, 2, 1], 'keyword_id': [1, 236627, 1], 'production_year': [1878, 2115, 1]} 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id!=1 AND mk.keyword_id<19712 AND t.production_year<1980;{};63056995;{'company_type_id': [1, 2, 1], 'keyword_id': [1, 236627, 1], 'production_year': [1878, 2115, 1]} 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id>1 AND mk.keyword_id<=186471 AND t.production_year<2110;{};63056995;{'company_type_id': [1, 2, 1], 'keyword_id': [1, 236627, 1], 'production_year': [1878, 2115, 1]} Alternativly also a SQL-file with just the queries would work as input, but be aware that you have to add the meta data later then for further processing. Initialize the Database Evaluator and call the get_cardinalities() function. python evaluator = DatabaseEvaluator(input_file_path=inter_file_path, database_connector=database_connector) evaluator.get_cardinalities(eliminate_null_queries=False, save_file_path=save_file_path.split(\".\")[0], query_number=query_number) Note, that inter_file_path holding just the same string as save_file_path with a additional \"inter_\"-praefix (file used in the SQL-Generator module). Per default all intermediate products are saved in the asset folder. Be also beware of the hints in the parent module (QueryCommunicator) documentation .","title":"Usage"},{"location":"querycommunicator/querycommunicator-class/sqlgenerator-class/","text":"","title":"SQLGenerator Class"},{"location":"querycommunicator/querycommunicator-class/sqlgenerator-readme/","text":"SQL Generator This is a sub(sub)module for genrating sql queries, given a meta_information file. It's typically called from the Query communicator , which manages the hole process of generating queries and evaluating them. However, you can use the geneartor seperately to genrate queries without duplicates. Usage To initialize the generator, you need a meta_information file in .yaml file format. The file contains the columns you want to join, the attributes you want to select, the stepsize and min/max values of the columns, and so on. You can have more than one entry to generate queries. Note, that in the end there will be an own model for every entry in the meta_file and query_set respectively. If you want to generate more than one query set, note that there has to be an own entry with own ID in the meta file the columns will need a synonym as second argument Example file 0: columns: - - company_type_id - mc - integer - - 1 - 2 - 1 - {} - - info_type_id - mi_idx - integer - - 1 - 113 - 1 - {} - - production_year - t - integer - - 1874 - 2115 - 1 - {} join_attributes: - t.id=mc.movie_id - t.id=mi_idx.movie_id max_card: - 134163798 table_names: - - movie_companies - mc - - movie_info_idx - mi_idx - - title - t Given the meta File: Instantiate a new sql-generator with meta File python generator = SQLGenarator(config='meta_information_test.yaml') Generate the desired number of queries. Note that the default is 10(queries per entry in meta file) python generator.generate_queries(qnumber=100, save_readable = 'queries') You also have the possibility to specify the filename for the queries. Note that you just specify the name (e.g.'movie_queries') and not the format. The Generator will safe a csv file (usable for vectorizer) and a human readable sql file with the specified name in the assets directory. Default is queries.csv/queries.sql .","title":"SQLGenerator Readme"},{"location":"querycommunicator/querycommunicator-class/sqlgenerator-readme/#sql-generator","text":"This is a sub(sub)module for genrating sql queries, given a meta_information file. It's typically called from the Query communicator , which manages the hole process of generating queries and evaluating them. However, you can use the geneartor seperately to genrate queries without duplicates.","title":"SQL Generator"},{"location":"querycommunicator/querycommunicator-class/sqlgenerator-readme/#usage","text":"To initialize the generator, you need a meta_information file in .yaml file format. The file contains the columns you want to join, the attributes you want to select, the stepsize and min/max values of the columns, and so on. You can have more than one entry to generate queries. Note, that in the end there will be an own model for every entry in the meta_file and query_set respectively. If you want to generate more than one query set, note that there has to be an own entry with own ID in the meta file the columns will need a synonym as second argument Example file 0: columns: - - company_type_id - mc - integer - - 1 - 2 - 1 - {} - - info_type_id - mi_idx - integer - - 1 - 113 - 1 - {} - - production_year - t - integer - - 1874 - 2115 - 1 - {} join_attributes: - t.id=mc.movie_id - t.id=mi_idx.movie_id max_card: - 134163798 table_names: - - movie_companies - mc - - movie_info_idx - mi_idx - - title - t Given the meta File: Instantiate a new sql-generator with meta File python generator = SQLGenarator(config='meta_information_test.yaml') Generate the desired number of queries. Note that the default is 10(queries per entry in meta file) python generator.generate_queries(qnumber=100, save_readable = 'queries') You also have the possibility to specify the filename for the queries. Note that you just specify the name (e.g.'movie_queries') and not the format. The Generator will safe a csv file (usable for vectorizer) and a human readable sql file with the specified name in the assets directory. Default is queries.csv/queries.sql .","title":"Usage"},{"location":"queryparser/queryparser-class/","text":"query_parser.query_parser QueryFormat Objects class QueryFormat(Enum) Enum for the different supported query-formats. CROSS_PRODUCT: SELECT COUNT(*) FROM movie_companies mc,title t,movie_info_idx mi_idx WHERE t.id=mc.movie_id AND t.id=mi_idx.movie_id AND mi_idx.info_type_id=112 AND mc.company_type_id=2; JOIN_ON: SELECT COUNT(*) FROM movie_companies mc INNER JOIN title t ON (t.id=mc.movie_id) INNER JOIN movie_info_idx mi_idx ON (t.id=mi_idx.movie_id) WHERE mi_idx.info_type_id=112 AND mc.company_type_id=2; QueryParser Objects class QueryParser() Class for the query_parser. This is responsible of reading a given file (.csv/.tsv or .sql) which contains sql queries (for more details see Readme) parse them and return a file (.yaml) containing the aggregated information of the input file. This aggregated .yaml file is the requirement for the MetaCollector. operators The possible operators which can occur in the queries. [\"<=\", \"!=\", \">=\", \"=\", \"<\", \">\", \"IS\"] read_file def read_file(file_path: str, inner_separator: str = None, outer_separator: str = None, query_format: QueryFormat = QueryFormat.CROSS_PRODUCT) -> Tuple[Dict, str, str, str] Generic method for rooting the processing of the input file which contains the queries according to the given file type. Because .sql/.tsv files need to be processed another way than .sql files. The parameters inner_separator and outer_separator allow the user to use customized .csv/.tsv files. The parameter query_format allows the user to choose between the two most common join formats. Arguments : .sql. No other file types are supported at the moment. This path could be absolute as well as relative. documentation for details. :return A tuple containing a dictionary with the table-string as key and a list of selection attributes as value, the file-type, the inner_separator and the outer_separator. - file_path : Path to the file containing the sql statements. This path has to end with .csv/.tsv or - inner_separator : The column separator used in the file. You can use '\\t' for .tsv files. -> See - outer_separator : The block separator used in the file. -> See documentation for details. - query_format : The format of the sql query. Look at documentation of QueryFormat for details. read_sql_file @staticmethod def read_sql_file(file_path: str, query_format: QueryFormat = QueryFormat.CROSS_PRODUCT) -> Tuple[Dict, str, str, str] Read and parse the sql statements from given sql file. The most parts of the sql syntax are processed and removed. Parts like 'SELECT COUNT(*)' and 'INNER JOIN' are removed from the query. Arguments : types are supported at the moment. This path could be absolute as well as relative. :return A tuple containing a dictionary with the table-string as key and a list of selection attributes as value, the file-type, the inner_separator and the outer_separator. - file_path : Path to the file containing the sql statements. This path has to end with .sql. No other file - query_format : The format of the sql query. Look at documentation of QueryFormat for details. read_csv_file @staticmethod def read_csv_file(file_path: str, inner_separator: str = \",\", outer_separator: str = \"#\") -> Tuple[Dict, str, str, str] Read the csv formatted sql statements from given file. For more details on the format, look at the readme. Arguments : with .csv or .tsv. No other file types are supported at the moment. This path could be absolute as well as relative. documentation for details. :return A tuple containing a dictionary with the table-string as key and a list of selection attributes as value, the file-type, the inner_separator and the outer_separator. - file_path : Path to the file containing the sql statements formatted as csv or .tsv. This path has to end - inner_separator : The column separator used in the file. You can use '\\t' for .tsv files. -> See - outer_separator : The block separator used in the file. -> See documentation for details. create_solution_dict def create_solution_dict(command_dict: Dict[str, List[str] or List[Tuple[str, str]]], file_type: str, inner_separator: str) -> Dict[int, Dict[str, List[str or Tuple[str, str]]]] Method for building the solution dict. Therefore the given file with the queries must be parsed at first and the command_dict must be created. Arguments : clauses as string if the file type is sql or a list of tuples containing the join-attribute-string in first and the selection-attribute-string in second place. documentation for details. :return The solution dict containing 'table_names', 'join_attributes' and 'selection_attributes'. - command_dict : Dict with a alphabetical sorted string of the joining tables as key and a list of where - file_type : String with 'csv'/'tsv' or 'sql' which tells the file type of the read file. - inner_separator : The column separator used in the file. You can use '\\t' for .tsv files. -> See table_name_unpacker @staticmethod def table_name_unpacker(from_string: str, separator: str = \",\") -> List[Tuple[str, str]] Takes the sorted string of the from clause and extracts the tables with their aliases. Arguments : from_string : Alphabetical ordered string containing all tables to join, separated by the separator. separator : The column separator used in the file. You can use '\\t' for .tsv files. Returns : List of tuples where the first element of the tuple is the table name and the second one is the alias. sql_attribute_unpacker def sql_attribute_unpacker(where_string_list: List[str]) -> Tuple[List[str], List[str]] Unpack the attribute strings from sql-file into sets containing the attributes. Arguments : selection-attributes. where_string_list : A list of strings from the where clauses. These have to be separated into join- and Returns : A tuple containing the list of join-attributes in first and the list of selection-attributes in second csv_attribute_unpacker def csv_attribute_unpacker(attribute_tuples: List[Tuple[str, str]], separator: str = \",\") -> Tuple[List[str], List[str]] Unpack the attribute strings from csv-file into sets containing the attributes. Arguments : join-attributes, while the second string contains all selection-attributes. attribute_tuples : A list of tuples of strings where the first string is the string for all separator : The column separator used in the file. You can use '\\t' for .tsv files. Returns : A tuple containing the list of join-attributes in first and the list of selection-attributes in second save_solution_dict @staticmethod def save_solution_dict(solution_dict: Dict[int, Dict[str, List[str or Tuple[str, str]]]], save_file_path: str = \"solution_dict\") Save the solution to file with specified filename. Arguments : automatically. - solution_dict : The dict containing the data to save. - save_file_path : The path for the file in which the data should be saved. The .yaml ending is added run def run(file_path: str, save_file_path: str, inner_separator: str = None, outer_separator: str = None, query_format: QueryFormat = QueryFormat.CROSS_PRODUCT) -> Dict[int, Dict[str, List[str or Tuple[str, str]]]] Method for the whole parsing process. Arguments : documentation for details. this is not used. file_path : The file to read in which the sql-statements are saved. save_file_path : The path where to save the results. inner_separator : The column separator used in the file. You can use '\\t' for .tsv files. -> See outer_separator : The block separator used in the file. -> See documentation for details. query_format : The indicator for the format of the .sql query-file. If the given file is not .sql than Returns :","title":"QueryParser Class"},{"location":"queryparser/queryparser-class/#query_parserquery_parser","text":"","title":"query_parser.query_parser"},{"location":"queryparser/queryparser-class/#queryformat-objects","text":"class QueryFormat(Enum) Enum for the different supported query-formats. CROSS_PRODUCT: SELECT COUNT(*) FROM movie_companies mc,title t,movie_info_idx mi_idx WHERE t.id=mc.movie_id AND t.id=mi_idx.movie_id AND mi_idx.info_type_id=112 AND mc.company_type_id=2; JOIN_ON: SELECT COUNT(*) FROM movie_companies mc INNER JOIN title t ON (t.id=mc.movie_id) INNER JOIN movie_info_idx mi_idx ON (t.id=mi_idx.movie_id) WHERE mi_idx.info_type_id=112 AND mc.company_type_id=2;","title":"QueryFormat Objects"},{"location":"queryparser/queryparser-class/#queryparser-objects","text":"class QueryParser() Class for the query_parser. This is responsible of reading a given file (.csv/.tsv or .sql) which contains sql queries (for more details see Readme) parse them and return a file (.yaml) containing the aggregated information of the input file. This aggregated .yaml file is the requirement for the MetaCollector.","title":"QueryParser Objects"},{"location":"queryparser/queryparser-class/#operators","text":"The possible operators which can occur in the queries. [\"<=\", \"!=\", \">=\", \"=\", \"<\", \">\", \"IS\"]","title":"operators"},{"location":"queryparser/queryparser-class/#read_file","text":"def read_file(file_path: str, inner_separator: str = None, outer_separator: str = None, query_format: QueryFormat = QueryFormat.CROSS_PRODUCT) -> Tuple[Dict, str, str, str] Generic method for rooting the processing of the input file which contains the queries according to the given file type. Because .sql/.tsv files need to be processed another way than .sql files. The parameters inner_separator and outer_separator allow the user to use customized .csv/.tsv files. The parameter query_format allows the user to choose between the two most common join formats. Arguments : .sql. No other file types are supported at the moment. This path could be absolute as well as relative. documentation for details. :return A tuple containing a dictionary with the table-string as key and a list of selection attributes as value, the file-type, the inner_separator and the outer_separator. - file_path : Path to the file containing the sql statements. This path has to end with .csv/.tsv or - inner_separator : The column separator used in the file. You can use '\\t' for .tsv files. -> See - outer_separator : The block separator used in the file. -> See documentation for details. - query_format : The format of the sql query. Look at documentation of QueryFormat for details.","title":"read_file"},{"location":"queryparser/queryparser-class/#read_sql_file","text":"@staticmethod def read_sql_file(file_path: str, query_format: QueryFormat = QueryFormat.CROSS_PRODUCT) -> Tuple[Dict, str, str, str] Read and parse the sql statements from given sql file. The most parts of the sql syntax are processed and removed. Parts like 'SELECT COUNT(*)' and 'INNER JOIN' are removed from the query. Arguments : types are supported at the moment. This path could be absolute as well as relative. :return A tuple containing a dictionary with the table-string as key and a list of selection attributes as value, the file-type, the inner_separator and the outer_separator. - file_path : Path to the file containing the sql statements. This path has to end with .sql. No other file - query_format : The format of the sql query. Look at documentation of QueryFormat for details.","title":"read_sql_file"},{"location":"queryparser/queryparser-class/#read_csv_file","text":"@staticmethod def read_csv_file(file_path: str, inner_separator: str = \",\", outer_separator: str = \"#\") -> Tuple[Dict, str, str, str] Read the csv formatted sql statements from given file. For more details on the format, look at the readme. Arguments : with .csv or .tsv. No other file types are supported at the moment. This path could be absolute as well as relative. documentation for details. :return A tuple containing a dictionary with the table-string as key and a list of selection attributes as value, the file-type, the inner_separator and the outer_separator. - file_path : Path to the file containing the sql statements formatted as csv or .tsv. This path has to end - inner_separator : The column separator used in the file. You can use '\\t' for .tsv files. -> See - outer_separator : The block separator used in the file. -> See documentation for details.","title":"read_csv_file"},{"location":"queryparser/queryparser-class/#create_solution_dict","text":"def create_solution_dict(command_dict: Dict[str, List[str] or List[Tuple[str, str]]], file_type: str, inner_separator: str) -> Dict[int, Dict[str, List[str or Tuple[str, str]]]] Method for building the solution dict. Therefore the given file with the queries must be parsed at first and the command_dict must be created. Arguments : clauses as string if the file type is sql or a list of tuples containing the join-attribute-string in first and the selection-attribute-string in second place. documentation for details. :return The solution dict containing 'table_names', 'join_attributes' and 'selection_attributes'. - command_dict : Dict with a alphabetical sorted string of the joining tables as key and a list of where - file_type : String with 'csv'/'tsv' or 'sql' which tells the file type of the read file. - inner_separator : The column separator used in the file. You can use '\\t' for .tsv files. -> See","title":"create_solution_dict"},{"location":"queryparser/queryparser-class/#table_name_unpacker","text":"@staticmethod def table_name_unpacker(from_string: str, separator: str = \",\") -> List[Tuple[str, str]] Takes the sorted string of the from clause and extracts the tables with their aliases. Arguments : from_string : Alphabetical ordered string containing all tables to join, separated by the separator. separator : The column separator used in the file. You can use '\\t' for .tsv files. Returns : List of tuples where the first element of the tuple is the table name and the second one is the alias.","title":"table_name_unpacker"},{"location":"queryparser/queryparser-class/#sql_attribute_unpacker","text":"def sql_attribute_unpacker(where_string_list: List[str]) -> Tuple[List[str], List[str]] Unpack the attribute strings from sql-file into sets containing the attributes. Arguments : selection-attributes. where_string_list : A list of strings from the where clauses. These have to be separated into join- and Returns : A tuple containing the list of join-attributes in first and the list of selection-attributes in second","title":"sql_attribute_unpacker"},{"location":"queryparser/queryparser-class/#csv_attribute_unpacker","text":"def csv_attribute_unpacker(attribute_tuples: List[Tuple[str, str]], separator: str = \",\") -> Tuple[List[str], List[str]] Unpack the attribute strings from csv-file into sets containing the attributes. Arguments : join-attributes, while the second string contains all selection-attributes. attribute_tuples : A list of tuples of strings where the first string is the string for all separator : The column separator used in the file. You can use '\\t' for .tsv files. Returns : A tuple containing the list of join-attributes in first and the list of selection-attributes in second","title":"csv_attribute_unpacker"},{"location":"queryparser/queryparser-class/#save_solution_dict","text":"@staticmethod def save_solution_dict(solution_dict: Dict[int, Dict[str, List[str or Tuple[str, str]]]], save_file_path: str = \"solution_dict\") Save the solution to file with specified filename. Arguments : automatically. - solution_dict : The dict containing the data to save. - save_file_path : The path for the file in which the data should be saved. The .yaml ending is added","title":"save_solution_dict"},{"location":"queryparser/queryparser-class/#run","text":"def run(file_path: str, save_file_path: str, inner_separator: str = None, outer_separator: str = None, query_format: QueryFormat = QueryFormat.CROSS_PRODUCT) -> Dict[int, Dict[str, List[str or Tuple[str, str]]]] Method for the whole parsing process. Arguments : documentation for details. this is not used. file_path : The file to read in which the sql-statements are saved. save_file_path : The path where to save the results. inner_separator : The column separator used in the file. You can use '\\t' for .tsv files. -> See outer_separator : The block separator used in the file. -> See documentation for details. query_format : The indicator for the format of the .sql query-file. If the given file is not .sql than Returns :","title":"run"},{"location":"queryparser/queryparser-readme/","text":"QueryParser This module parses a given query file and extracts the information for the joins. Based on the different joins the QueryParser creates different QuerySets with their QuerySetIDs. Usage Normally this module is called from main.py , however you may want to use it separately: You need a file containing sql queries. There are some possibilities to format such a file: 1.1 An sql file can have the following two formats: SELECT COUNT(*) FROM movie_companies mc,title t,movie_info_idx mi_idx WHERE t.id=mc.movie_id AND t.id=mi_idx.movie_id AND mi_idx.info_type_id=112 AND mc.company_type_id=2; or SELECT COUNT(*) From movie_companies mc INNER JOIN title t On (t.id=mc.movie_id) Inner JOIN movie_info_idx mi_idx on (t.id=mi_idx.movie_id) WHERE mi_idx.info_type_id=112 AND mc.company_type_id=2; 1.2 A csv file can have the following formats (Where '#' is the outer_separator and ',' the inner_separator. These separators can be customized): movie_companies mc,title t,movie_info_idx mi_idx#t.id=mc.movie_id,t.id=mi_idx.movie_id#mi_idx.info_type_id,=,112,mc.company_type_id,=,2#715 1.3 A tsv file is the same as a csv file, but with tab or '\\t' as inner_separator. The result is saved as .yaml file like: 0: join_attributes: - t.id=mc.movie_id - t.id=mi_idx.movie_id selection_attributes: - mc.company_type_id - mi_idx.info_type_id - t.production_year table_names: - - movie_companies - mc - - movie_info_idx - mi_idx - - title - t 1: join_attributes: - t.id=mc.movie_id - t.id=mk.movie_id selection_attributes: - mc.company_type_id - mk.keyword_id - t.production_year table_names: - - movie_companies - mc - - movie_keyword - mk - - title - t","title":"QueryParser Readme"},{"location":"queryparser/queryparser-readme/#queryparser","text":"This module parses a given query file and extracts the information for the joins. Based on the different joins the QueryParser creates different QuerySets with their QuerySetIDs.","title":"QueryParser"},{"location":"queryparser/queryparser-readme/#usage","text":"Normally this module is called from main.py , however you may want to use it separately: You need a file containing sql queries. There are some possibilities to format such a file: 1.1 An sql file can have the following two formats: SELECT COUNT(*) FROM movie_companies mc,title t,movie_info_idx mi_idx WHERE t.id=mc.movie_id AND t.id=mi_idx.movie_id AND mi_idx.info_type_id=112 AND mc.company_type_id=2; or SELECT COUNT(*) From movie_companies mc INNER JOIN title t On (t.id=mc.movie_id) Inner JOIN movie_info_idx mi_idx on (t.id=mi_idx.movie_id) WHERE mi_idx.info_type_id=112 AND mc.company_type_id=2; 1.2 A csv file can have the following formats (Where '#' is the outer_separator and ',' the inner_separator. These separators can be customized): movie_companies mc,title t,movie_info_idx mi_idx#t.id=mc.movie_id,t.id=mi_idx.movie_id#mi_idx.info_type_id,=,112,mc.company_type_id,=,2#715 1.3 A tsv file is the same as a csv file, but with tab or '\\t' as inner_separator. The result is saved as .yaml file like: 0: join_attributes: - t.id=mc.movie_id - t.id=mi_idx.movie_id selection_attributes: - mc.company_type_id - mi_idx.info_type_id - t.production_year table_names: - - movie_companies - mc - - movie_info_idx - mi_idx - - title - t 1: join_attributes: - t.id=mc.movie_id - t.id=mk.movie_id selection_attributes: - mc.company_type_id - mk.keyword_id - t.production_year table_names: - - movie_companies - mc - - movie_keyword - mk - - title - t","title":"Usage"},{"location":"vectorizer/vectorizer-class/","text":"vectorizer.vectorizer Vectorizer Objects class Vectorizer() Constructs a vector consisting of operator code and normalized value for each predicate in the sql query set with set_query method. __init__ def __init__() Intitialises the Vectorizer object by defining available operators. add_queries_with_cardinalities def add_queries_with_cardinalities(queries_with_cardinalities_path: str) Reads CSV file with format (querySetID;query;encodings;max_card;min_max_step;estimated_cardinality;true_cardinality) whereas min_max_step is an array of the format [[1, 2, 1], [1, 113, 1], [1878, 2115, 1]] sorted by lexicographic order of corresponding predicates and encodings is an empty array if only integer values are processed. For a querySetID all predicates are collected and sorted in lexicographical order to provide correct indices (e.g. in encodings & min_max_value) for a given predicate. Read queries are added to the list of vectorisation tasks. Arguments : true cardinalities - queries_with_cardinalities_path : path to a CSV file containing all queries and their estimated and vectorize def vectorize() -> List[np.array] Vectorizes all vectorization tasks added. Returns : List of np.array vectors whereas each row contains the vectorized query and appended maximal, save def save(base_path: str, result_folder: str, base_filename: str, filetypes: str) Stores the SQL query and corresponding vector at given path as NPY and TXT file. Arguments : base_path to empathize the need for an extra folder, since multiple files are saved. base_path : path to a directory for saving result_folder : name of folder to create for storing multiple files. This argument is seperated from filename : filename without filetype. querySetID is appended for differentiation filetypes : string of file types must contain \"csv\" or \"npy\" vectorize_query_original def vectorize_query_original(query: str, min_max: Dict[str, Tuple[int, int, int]], encoders: List[Dict[str, int]]) -> np.array Copy-pasted method of the original implementation for testing purposes; Only added Join detection Arguments : query : the query to vectorize min_max : dictionary of all min, max, step values for each predicate encoders : dictionary, which maps predicates to encoders Returns : the normalized vector without cardinalities vectorizer_tests def vectorizer_tests() Test method to compare the original implementation with jupyter notebook output (truth) or with the Vectorizer implementation. Succeeds if no assertion throws an error.","title":"Vectorizer Class"},{"location":"vectorizer/vectorizer-class/#vectorizervectorizer","text":"","title":"vectorizer.vectorizer"},{"location":"vectorizer/vectorizer-class/#vectorizer-objects","text":"class Vectorizer() Constructs a vector consisting of operator code and normalized value for each predicate in the sql query set with set_query method.","title":"Vectorizer Objects"},{"location":"vectorizer/vectorizer-class/#__init__","text":"def __init__() Intitialises the Vectorizer object by defining available operators.","title":"__init__"},{"location":"vectorizer/vectorizer-class/#add_queries_with_cardinalities","text":"def add_queries_with_cardinalities(queries_with_cardinalities_path: str) Reads CSV file with format (querySetID;query;encodings;max_card;min_max_step;estimated_cardinality;true_cardinality) whereas min_max_step is an array of the format [[1, 2, 1], [1, 113, 1], [1878, 2115, 1]] sorted by lexicographic order of corresponding predicates and encodings is an empty array if only integer values are processed. For a querySetID all predicates are collected and sorted in lexicographical order to provide correct indices (e.g. in encodings & min_max_value) for a given predicate. Read queries are added to the list of vectorisation tasks. Arguments : true cardinalities - queries_with_cardinalities_path : path to a CSV file containing all queries and their estimated and","title":"add_queries_with_cardinalities"},{"location":"vectorizer/vectorizer-class/#vectorize","text":"def vectorize() -> List[np.array] Vectorizes all vectorization tasks added. Returns : List of np.array vectors whereas each row contains the vectorized query and appended maximal,","title":"vectorize"},{"location":"vectorizer/vectorizer-class/#save","text":"def save(base_path: str, result_folder: str, base_filename: str, filetypes: str) Stores the SQL query and corresponding vector at given path as NPY and TXT file. Arguments : base_path to empathize the need for an extra folder, since multiple files are saved. base_path : path to a directory for saving result_folder : name of folder to create for storing multiple files. This argument is seperated from filename : filename without filetype. querySetID is appended for differentiation filetypes : string of file types must contain \"csv\" or \"npy\"","title":"save"},{"location":"vectorizer/vectorizer-class/#vectorize_query_original","text":"def vectorize_query_original(query: str, min_max: Dict[str, Tuple[int, int, int]], encoders: List[Dict[str, int]]) -> np.array Copy-pasted method of the original implementation for testing purposes; Only added Join detection Arguments : query : the query to vectorize min_max : dictionary of all min, max, step values for each predicate encoders : dictionary, which maps predicates to encoders Returns : the normalized vector without cardinalities","title":"vectorize_query_original"},{"location":"vectorizer/vectorizer-class/#vectorizer_tests","text":"def vectorizer_tests() Test method to compare the original implementation with jupyter notebook output (truth) or with the Vectorizer implementation. Succeeds if no assertion throws an error.","title":"vectorizer_tests"},{"location":"vectorizer/vectorizer-readme/","text":"Vectorizer This submodule uses the output of the postgres-evaluater submodule to encode the SQL query into a vector and also normalizes the cardinalities given. Vector and cardinalities are input to the estimator submodule. Usage Normally this submodule is called from main.py , however you may want to use it separately: First, you need a CSV file (semicolon separated) with the queries, meta data and estimated and true cardinalities. E.g. queries_with_cardinalities.csv : querySetID;query;encodings;max_card;min_max_step;estimated_cardinality;true_cardinality 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id<2 AND mi_idx.info_type_id=107 AND t.production_year>2009;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];197595;1152438 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id=1 AND mi_idx.info_type_id<80 AND t.production_year<=1894;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];30903;1416 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id<=1 AND mi_idx.info_type_id!=62 AND t.production_year<=2094;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];70814563;94395272 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id>=2 AND mi_idx.info_type_id>45 AND t.production_year<1939;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];652856;197336 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id=2 AND mi_idx.info_type_id<=32 AND t.production_year<=1918;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];1733520;1054332 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id>=2 AND mi_idx.info_type_id<54 AND t.production_year<=2097;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];81368212;30387086 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id<=2 AND mi_idx.info_type_id>=38 AND t.production_year<1896;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];362;2501 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id=2 AND mi_idx.info_type_id<=66 AND t.production_year=2026;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];42743;14 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id=1 AND mk.keyword_id>=117023 AND t.production_year<=1894;[];63056995;[[1, 2, 1], [1, 236627, 1], [1878, 2115, 1]];436;26 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id>=1 AND mk.keyword_id<35239 AND t.production_year<=1896;[];63056995;[[1, 2, 1], [1, 236627, 1], [1878, 2115, 1]];18935;2117 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id<2 AND mk.keyword_id<=35888 AND t.production_year!=2020;[];63056995;[[1, 2, 1], [1, 236627, 1], [1878, 2115, 1]];14700094;40290318 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id=2 AND mk.keyword_id>196933 AND t.production_year=1907;[];63056995;[[1, 2, 1], [1, 236627, 1], [1878, 2115, 1]];101;2 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id!=1 AND mk.keyword_id<19712 AND t.production_year<1980;[];63056995;[[1, 2, 1], [1, 236627, 1], [1878, 2115, 1]];2728149;1552444 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id>1 AND mk.keyword_id<=186471 AND t.production_year<2110;[];63056995;[[1, 2, 1], [1, 236627, 1], [1878, 2115, 1]];24858350;14307838 encodings and min_max_step are arrays in string representation querySetID is mandatory, since it is used for automatically determine the length of the output vector. All vectors of the same querySet must have the same length. Given one or more such CSV files: Instantiate a new vectorizer python vectorizer = Vectorizer() Add as many CSV files with queries, meta data and cardinalities as you want python vectorizer.add_queries_with_cardinalities(\"queries_with_cardinalities_1.csv\") vectorizer.add_queries_with_cardinalities(\"queries_with_cardinalities_2.csv\") Vectorize all queries within the CSV files and normalize the cardinalities python vectors = vectorizer.vectorize() The resulting matrix contains for each row the vector, normalized estimated cardinality and normalized true cardinality python for vec in vectors: vectorized_query, cardinality_estimation, cardinality_true = vec[:-2], vec[-2], vec[-1] You may now want to save the matrix as .npy and .csv file python vectorizer.save(\"/path/to/directory/matrix\", \"csv\") E.g.: matrix.csv : 0,1,0,0,1,0,0,1,0.946902654867256666,0,1,0,0.554621848739495826,134163798,0.651576470484740322,0.745803338052605902 0,0,0,1,0.5,1,0,0,0.707964601769911495,1,0,1,0.0714285714285714246,134163798,0.552436280887511844,0.387697419969840307 0,1,0,1,0.5,1,1,0,0.548672566371681381,1,0,1,0.911764705882352922,134163798,0.9658556575333751,0.981214080678194711 0,0,1,1,1,0,1,0,0.39823008849557523,1,0,0,0.260504201680672287,134163798,0.715437781548679874,0.651506384900475854 0,0,0,1,1,1,0,1,0.283185840707964598,1,0,1,0.172268907563025209,134163798,0.767619189647782418,0.7410491653476593 0,0,1,1,1,1,0,0,0.477876106194690287,1,0,1,0.924369747899159711,134163798,0.973278750577822982,0.920647733098342469 0,1,0,1,1,0,1,1,0.336283185840707988,1,0,0,0.0798319327731092376,134163798,0.314815867372580604,0.418093768713123426 0,0,0,1,1,1,0,1,0.584070796460177011,0,0,1,0.626050420168067223,134163798,0.569767811257714807,0.141016173481957524 1,0,0,1,0.5,0,1,1,0.494546269022554497,1,0,1,0.0714285714285714246,63056995,0.338407275959206999,0.181413043100808496 1,0,1,1,0.5,1,0,0,0.148922143288804737,1,0,1,0.0798319327731092376,63056995,0.548386100028571355,0.426389049816630394 1,1,0,0,1,1,0,1,0.151664856504118289,1,1,0,0.600840336134453756,63056995,0.918918617255151005,0.975059073360462714 1,0,0,1,1,0,1,0,0.832250757521331042,0,0,1,0.126050420168067223,63056995,0.256973066165060549,0.0385949089828031763 1,1,1,0,0.5,1,0,0,0.0833041030820658723,1,0,0,0.432773109243697496,63056995,0.825139509620369638,0.793747135768320677 1,0,1,0,0.5,1,0,1,0.7880377133632257,1,0,0,0.978991596638655426,63056995,0.948169897872308987,0.917412655803315658 * Whole code: python vectorizer = Vectorizer() vectorizer.add_queries_with_cardinalities(\"queries_with_cardinalities_1.csv\") vectorizer.add_queries_with_cardinalities(\"queries_with_cardinalities_2.csv\") vectors = vectorizer.vectorize() for vec in vectors: vectorized_query, cardinality_estimation, cardinality_true = vec[:-2], vec[-2], vec[-1] vectorizer.save(\"/path/to/directory/filename\", \"csv\")","title":"Vectorizer Readme"},{"location":"vectorizer/vectorizer-readme/#vectorizer","text":"This submodule uses the output of the postgres-evaluater submodule to encode the SQL query into a vector and also normalizes the cardinalities given. Vector and cardinalities are input to the estimator submodule.","title":"Vectorizer"},{"location":"vectorizer/vectorizer-readme/#usage","text":"Normally this submodule is called from main.py , however you may want to use it separately: First, you need a CSV file (semicolon separated) with the queries, meta data and estimated and true cardinalities. E.g. queries_with_cardinalities.csv : querySetID;query;encodings;max_card;min_max_step;estimated_cardinality;true_cardinality 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id<2 AND mi_idx.info_type_id=107 AND t.production_year>2009;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];197595;1152438 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id=1 AND mi_idx.info_type_id<80 AND t.production_year<=1894;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];30903;1416 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id<=1 AND mi_idx.info_type_id!=62 AND t.production_year<=2094;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];70814563;94395272 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id>=2 AND mi_idx.info_type_id>45 AND t.production_year<1939;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];652856;197336 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id=2 AND mi_idx.info_type_id<=32 AND t.production_year<=1918;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];1733520;1054332 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id>=2 AND mi_idx.info_type_id<54 AND t.production_year<=2097;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];81368212;30387086 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id<=2 AND mi_idx.info_type_id>=38 AND t.production_year<1896;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];362;2501 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id=2 AND mi_idx.info_type_id<=66 AND t.production_year=2026;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];42743;14 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id=1 AND mk.keyword_id>=117023 AND t.production_year<=1894;[];63056995;[[1, 2, 1], [1, 236627, 1], [1878, 2115, 1]];436;26 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id>=1 AND mk.keyword_id<35239 AND t.production_year<=1896;[];63056995;[[1, 2, 1], [1, 236627, 1], [1878, 2115, 1]];18935;2117 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id<2 AND mk.keyword_id<=35888 AND t.production_year!=2020;[];63056995;[[1, 2, 1], [1, 236627, 1], [1878, 2115, 1]];14700094;40290318 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id=2 AND mk.keyword_id>196933 AND t.production_year=1907;[];63056995;[[1, 2, 1], [1, 236627, 1], [1878, 2115, 1]];101;2 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id!=1 AND mk.keyword_id<19712 AND t.production_year<1980;[];63056995;[[1, 2, 1], [1, 236627, 1], [1878, 2115, 1]];2728149;1552444 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id>1 AND mk.keyword_id<=186471 AND t.production_year<2110;[];63056995;[[1, 2, 1], [1, 236627, 1], [1878, 2115, 1]];24858350;14307838 encodings and min_max_step are arrays in string representation querySetID is mandatory, since it is used for automatically determine the length of the output vector. All vectors of the same querySet must have the same length. Given one or more such CSV files: Instantiate a new vectorizer python vectorizer = Vectorizer() Add as many CSV files with queries, meta data and cardinalities as you want python vectorizer.add_queries_with_cardinalities(\"queries_with_cardinalities_1.csv\") vectorizer.add_queries_with_cardinalities(\"queries_with_cardinalities_2.csv\") Vectorize all queries within the CSV files and normalize the cardinalities python vectors = vectorizer.vectorize() The resulting matrix contains for each row the vector, normalized estimated cardinality and normalized true cardinality python for vec in vectors: vectorized_query, cardinality_estimation, cardinality_true = vec[:-2], vec[-2], vec[-1] You may now want to save the matrix as .npy and .csv file python vectorizer.save(\"/path/to/directory/matrix\", \"csv\") E.g.: matrix.csv : 0,1,0,0,1,0,0,1,0.946902654867256666,0,1,0,0.554621848739495826,134163798,0.651576470484740322,0.745803338052605902 0,0,0,1,0.5,1,0,0,0.707964601769911495,1,0,1,0.0714285714285714246,134163798,0.552436280887511844,0.387697419969840307 0,1,0,1,0.5,1,1,0,0.548672566371681381,1,0,1,0.911764705882352922,134163798,0.9658556575333751,0.981214080678194711 0,0,1,1,1,0,1,0,0.39823008849557523,1,0,0,0.260504201680672287,134163798,0.715437781548679874,0.651506384900475854 0,0,0,1,1,1,0,1,0.283185840707964598,1,0,1,0.172268907563025209,134163798,0.767619189647782418,0.7410491653476593 0,0,1,1,1,1,0,0,0.477876106194690287,1,0,1,0.924369747899159711,134163798,0.973278750577822982,0.920647733098342469 0,1,0,1,1,0,1,1,0.336283185840707988,1,0,0,0.0798319327731092376,134163798,0.314815867372580604,0.418093768713123426 0,0,0,1,1,1,0,1,0.584070796460177011,0,0,1,0.626050420168067223,134163798,0.569767811257714807,0.141016173481957524 1,0,0,1,0.5,0,1,1,0.494546269022554497,1,0,1,0.0714285714285714246,63056995,0.338407275959206999,0.181413043100808496 1,0,1,1,0.5,1,0,0,0.148922143288804737,1,0,1,0.0798319327731092376,63056995,0.548386100028571355,0.426389049816630394 1,1,0,0,1,1,0,1,0.151664856504118289,1,1,0,0.600840336134453756,63056995,0.918918617255151005,0.975059073360462714 1,0,0,1,1,0,1,0,0.832250757521331042,0,0,1,0.126050420168067223,63056995,0.256973066165060549,0.0385949089828031763 1,1,1,0,0.5,1,0,0,0.0833041030820658723,1,0,0,0.432773109243697496,63056995,0.825139509620369638,0.793747135768320677 1,0,1,0,0.5,1,0,1,0.7880377133632257,1,0,0,0.978991596638655426,63056995,0.948169897872308987,0.917412655803315658 * Whole code: python vectorizer = Vectorizer() vectorizer.add_queries_with_cardinalities(\"queries_with_cardinalities_1.csv\") vectorizer.add_queries_with_cardinalities(\"queries_with_cardinalities_2.csv\") vectors = vectorizer.vectorize() for vec in vectors: vectorized_query, cardinality_estimation, cardinality_true = vec[:-2], vec[-2], vec[-1] vectorizer.save(\"/path/to/directory/filename\", \"csv\")","title":"Usage"}]}